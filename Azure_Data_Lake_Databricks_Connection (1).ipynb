{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5bf85fa-a44e-4390-9cf8-85b9f342ba9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8aca958-431a-425c-ade1-f17b61d50c63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"olistdatastoragacchet\" # Storage account name where the data is stored\n",
    "container = \"olistdata\" # Container name where the data is stored\n",
    "\n",
    "spark.conf.set( # Set the storage account key\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    {secretKey} # Secret key which user would be getting from the secret scope\n",
    ")\n",
    "\n",
    "base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/\" # Path to the bronze container where the raw data will be stored \n",
    "\n",
    "orders_path = base_path + \"olist_orders_dataset.csv\" # Path insidebronze container where olist_orders_dataset.csv will be stored\n",
    "payments_path = base_path + \"olist_order_payments_dataset.csv\" # Path inside bronze container where olist_order_payments_dataset.csv will be stored\n",
    "reviews_path = base_path + \"olist_order_reviews_dataset.csv\" # Path inside bronze container where olist_order_reviews_dataset.csv will be stored\n",
    "items_path = base_path + \"olist_order_items_dataset.csv\" # Path inside bronze container where olist_order_items_dataset.csv will be stored\n",
    "customers_path = base_path + \"olist_customers_dataset.csv\" # Path inside bronze container where olist_customers_dataset.csv will be stored\n",
    "sellers_path = base_path + \"olist_sellers_dataset.csv\" # Path inside bronze container where olist_sellers_dataset.csv will be stored\n",
    "geolocation_path = base_path + \"olist_geolocation_dataset.csv\" # Path inside bronze container where olist_geolocation_dataset.csv will be stored\n",
    "products_path = base_path + \"olist_products_dataset.csv\" # Path inside bronze container where olist_products_dataset.csv will be stored\n",
    "\n",
    "# Read the csv files with options of header as true(1st row is a column names) and store it in a dataframe\n",
    "orders_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(orders_path) \n",
    "payments_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(payments_path)\n",
    "reviews_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(reviews_path)\n",
    "items_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(items_path)\n",
    "customers_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(customers_path)\n",
    "sellers_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(sellers_path)\n",
    "geolocation_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(geolocation_path)\n",
    "products_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(products_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6abb44-90bc-4a5d-be84-3e9332b7549f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764354307193}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://olistdata@olistdatastoragacchet.dfs.core.windows.net/bronze/olist_customers_dataset.csv\") # Read the csv file with options of header as true(1st row is a column names) and store it in a dataframe\n",
    "\n",
    "display(df) # Display the dataframe df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f07ecd0-2b7d-45e2-8dba-f9c3465c6226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Reading Data from MongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6d397e77-f2a1-45d0-8160-5e6c17994a70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502b09a6-f589-4944-a46d-8b0206a5b40e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#importing module\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Credentials of connecting MongoDB dataset stored in files.io \n",
    "hostname=\"\"\n",
    "database = \"\"\n",
    "port=\"\"\n",
    "username = \"\"\n",
    "password=\"\"\n",
    "\n",
    "uri = \"mongodb://\" + username+\":\"+ password + \"@\" + hostname + \":\" + port + \"/\" + database\n",
    "\n",
    "# Connect with the port number and host\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# Access database\n",
    "mydatabase = client [database]\n",
    "\n",
    "mydatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b502d3d1-badc-48da-b1ce-b922054c0357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # Importing pandas module\n",
    "\n",
    "collection = mydatabase['product_categories'] # Accessing the collection\n",
    "mongo_data = pd.DataFrame(list(collection.find())) # Converting the collection into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c21800-c0d3-4efb-b781-30a7761be064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mongo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "349367c7-2db2-4ef6-bc05-701de396fe89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64267811-dd90-4b00-81cf-73750ad3489e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.get(scope=\"ecom-secret-scope\", key=\"ecom-secret-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772e8e65-7451-42ca-a664-3fe41f0f07f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, datediff, current_date # Importing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6e28ee-d198-44ca-8381-535961ce5f70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_dataframe(df, name): # Function to clean the dataframe\n",
    "    print(\"Cleaning \" + name) # Print the name of the dataframe\n",
    "    return df.drop_duplicates().na.drop(\"all\") # Return the dataframe with duplicates dropped and all null values dropped\n",
    "\n",
    "orders_df = clean_dataframe(orders_df, \"Orders\") # Call the function to clean the dataframe\n",
    "display(orders_df) # Display the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba4c3cb-67da-44f9-9aeb-f928d6caa086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Converting Date Columns\n",
    "\n",
    "from pyspark.sql.functions import to_date, col # import functions\n",
    "\n",
    "# Converting string columns to date type for accurate date calculations and comparisons\n",
    "orders_df = orders_df.withColumn(\"order_purchase_timestamp\", to_date(col(\"order_purchase_timestamp\")))\\\n",
    "    .withColumn(\"order_delivered_customer_date\", to_date(col(\"order_delivered_customer_date\")))\\\n",
    "        .withColumn(\"order_estimated_delivery_date\", to_date(col(\"order_estimated_delivery_date\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972c5f9c-07ff-4039-8ec4-7b1d34de9eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculating Delivery and Time delays\n",
    "from pyspark.sql.functions import datediff, col\n",
    "\n",
    "orders_df = orders_df.withColumn(\"actual_delivery_time\", datediff(\"order_delivered_customer_date\", \"order_purchase_timestamp\")) # Calculating the actual delivery time\n",
    "\n",
    "orders_df = orders_df.withColumn(\"estimated_delivery_time\", datediff(\"order_estimated_delivery_date\", \"order_purchase_timestamp\")) # Calculating the estimated delivery time\n",
    "\n",
    "orders_df = orders_df.withColumn(\"delay Time\",col(\"actual_delivery_time\") - col(\"estimated_delivery_time\")) # Calculating the delay time\n",
    "\n",
    "display(orders_df) # Displaying the dataframe\n",
    "                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d5606e-4394-4777-a895-4b5d7cfc76f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3faedcc-dce6-4e0e-b512-52e638927b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Performing joins between the dataframes\n",
    "\n",
    "orders_customers_df = orders_df.join(customers_df, orders_df.customer_id == customers_df.customer_id, \"left\") # Joining the orders and customers dataframes\n",
    "\n",
    "orders_payments_df = orders_customers_df.join(payments_df, orders_customers_df.order_id == payments_df.order_id, \"left\") # Joining the orders and payments dataframes\n",
    "\n",
    "orders_items_df = orders_payments_df.join(items_df, \"order_id\", \"left\") # Joining the orders and items dataframes\n",
    "\n",
    "orders_items_products_df = orders_items_df.join(products_df, orders_items_df.product_id == products_df.product_id, \"left\") # Joining the orders, items and products dataframes\n",
    "\n",
    "final_df = orders_items_products_df.join(sellers_df, orders_items_products_df.seller_id == sellers_df.seller_id, \"left\") # Joining the final dataframe with the sellers dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab216a3-a6c3-4d22-abfe-0adc14b496ac",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764551814513}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df) # Displaying the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c43e3411-9dd2-49ea-9aaf-0f0bdcff251f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mongo_data.drop(\"_id\", axis=1, inplace=True) # Dropping the _id column which mongoDB has for all the json objects stored.\n",
    "\n",
    "mongo_spark_df = spark.createDataFrame(mongo_data) # Creating a spark dataframe from the mongo dataframe\n",
    "\n",
    "display(mongo_spark_df) # Displaying the mongo spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89187fb6-1dd5-4ad0-89c9-9a946a76fe39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = final_df.join(mongo_spark_df, \"product_category_name\", \"left\") # Joining the final dataframe with the mongo spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2bf078a-e068-4a14-958f-8fababf61ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df) # Displaying the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be96323-d942-4ac2-a666-efecb0908134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_columns(df): # Function to remove duplicate \n",
    "    columns = df.columns # Getting the columns \n",
    "\n",
    "    seen_columns = set() # Creating a set to store the columns seen\n",
    "    columns_to_drop = [] # Creating a list to store the columns to drop\n",
    "\n",
    "    for column in columns: # Looping through the columns\n",
    "        if column in seen_columns: # Checking if the column is\n",
    "            columns_to_drop.append(column) # Adding the column to the list\n",
    "        else:\n",
    "            seen_columns.add(column) # Adding the column to the set\n",
    "            \n",
    "    df_cleaned = df.drop(*columns_to_drop) # Dropping the columns\n",
    "    return df_cleaned # Returning the dataframe\n",
    "\n",
    "final_df = remove_duplicate_columns(final_df) # Calling the function to remove duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0fda23-d7b4-436d-b541-01514f87de72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing the final_df Spark DataFrame to the specified Azure Data Lake Storage Gen2 path in Parquet format,\n",
    "# using 'overwrite' mode to replace any existing data at the destination\n",
    "# Cleaned data is stored in silver container path as it is business ready data\n",
    "#  \n",
    "final_df.write.mode(\"overwrite\").parquet(\"abfss://olistdata@olistdatastoragacchet.dfs.core.windows.net/silver\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Azure_Data_Lake_Databricks_Connection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}